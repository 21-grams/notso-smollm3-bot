# Complete System Architecture & Call Chains

## Table of Contents
1. [Server Startup Call Chain](#server-startup-call-chain)
2. [Inference Engine Call Chain](#inference-engine-call-chain)
3. [UI Input/Output Call Chain](#ui-inputoutput-call-chain)
4. [Broadcast SSE Architecture](#broadcast-sse-architecture)
5. [Error Handling & Fallback Chains](#error-handling--fallback-chains)

---

## Server Startup Call Chain

### Complete Startup Sequence

```
main.rs::main()
â”‚
â”œâ”€â”€ tracing_subscriber::registry()
â”‚   â””â”€â”€ .with(EnvFilter::try_from_default_env())
â”‚   â””â”€â”€ .with(fmt::layer())
â”‚   â””â”€â”€ .init()
â”‚
â”œâ”€â”€ tracing::info!("ğŸš€ Starting SmolLM3 Bot Server")
â”‚
â”œâ”€â”€ AppState::new().await?
â”‚   â”‚
â”‚   â”œâ”€â”€ Config::from_env()?
â”‚   â”‚   â”œâ”€â”€ Read HOST (default: "127.0.0.1")
â”‚   â”‚   â”œâ”€â”€ Read PORT (default: "3000")
â”‚   â”‚   â”œâ”€â”€ Read MODEL_PATH (default: "models/SmolLM3-3B-Q4_K_M.gguf")
â”‚   â”‚   â”œâ”€â”€ Read TOKENIZER_PATH (default: "models/tokenizer.json")
â”‚   â”‚   â””â”€â”€ Set device: DeviceConfig::Cpu
â”‚   â”‚
â”‚   â”œâ”€â”€ MLService::new() [OPTIONAL - Non-blocking]
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ SmolLM3Model::from_gguf(model_path, &device)?
â”‚   â”‚   â”‚   â”œâ”€â”€ gguf_file::Content::read(&mut file)?
â”‚   â”‚   â”‚   â”œâ”€â”€ Load metadata and tensors
â”‚   â”‚   â”‚   â”œâ”€â”€ Initialize quantized weights
â”‚   â”‚   â”‚   â””â”€â”€ Return SmolLM3Model
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ SmolLM3Tokenizer::from_file(tokenizer_path)?
â”‚   â”‚   â”‚   â”œâ”€â”€ Read tokenizer.json
â”‚   â”‚   â”‚   â”œâ”€â”€ Parse vocabulary
â”‚   â”‚   â”‚   â””â”€â”€ Setup special tokens
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ SmolLM3KVCache::new(num_layers, max_seq_len, device)
â”‚   â”‚   â”‚   â””â”€â”€ Initialize empty cache layers
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ LogitsProcessor::new(seed, temperature, top_p)
â”‚   â”‚
â”‚   â”œâ”€â”€ [On MLService Error]
â”‚   â”‚   â”œâ”€â”€ tracing::warn!("âš ï¸ Model not available: {}", e)
â”‚   â”‚   â”œâ”€â”€ tracing::info!("ğŸŒ Server will start without model")
â”‚   â”‚   â””â”€â”€ Set model = None
â”‚   â”‚
â”‚   â”œâ”€â”€ TemplateEngine::new()?
â”‚   â”‚   â””â”€â”€ Initialize MiniJinja templates
â”‚   â”‚
â”‚   â””â”€â”€ Return AppState {
â”‚       config: Arc::new(config),
â”‚       model: Arc::new(RwLock::new(ml_service)), // Option<MLService>
â”‚       sessions: Arc::new(RwLock::new(SessionManager::new())),
â”‚       templates: Arc::new(templates),
â”‚   }
â”‚
â””â”€â”€ web::start_server(state).await?
    â”‚
    â”œâ”€â”€ let app = create_app(state.clone())
    â”‚   â”‚
    â”‚   â”œâ”€â”€ Router::new()
    â”‚   â”‚   â”œâ”€â”€ .route("/", get(handlers::chat::index))
    â”‚   â”‚   â”œâ”€â”€ .route("/api/chat", post(handlers::api::send_message))
    â”‚   â”‚   â”œâ”€â”€ .route("/api/stream/{session_id}", get(handlers::api::stream_session))
    â”‚   â”‚   â”œâ”€â”€ .route("/api/toggle-thinking", post(handlers::api::toggle_thinking))
    â”‚   â”‚   â”œâ”€â”€ .route("/test-sse", get(handlers::api::test_sse))
    â”‚   â”‚   â””â”€â”€ ... (other routes)
    â”‚   â”‚
    â”‚   â”œâ”€â”€ .nest_service("/static", ServeDir::new("src/web/static"))
    â”‚   â”‚
    â”‚   â””â”€â”€ .layer(ServiceBuilder::new()
    â”‚       â”œâ”€â”€ .layer(CorsLayer::permissive())
    â”‚       â””â”€â”€ .layer(TraceLayer::new_for_http()))
    â”‚
    â”œâ”€â”€ let addr = SocketAddr::from(([0, 0, 0, 0], state.config.port))
    â”‚
    â”œâ”€â”€ tracing::info!("ğŸŒ Server listening on http://{}", addr)
    â”‚
    â””â”€â”€ axum::Server::bind(&addr)
        .serve(app.into_make_service())
        .await?
```

---

## Inference Engine Call Chain

### Model Generation Flow (When Model Available)

```
generate_response_buffered(state, session_id, message, message_id)
â”‚
â”œâ”€â”€ Get broadcast sender
â”‚   â””â”€â”€ sessions.get_or_create_sender(&session_id)
â”‚       â”œâ”€â”€ Check if session exists
â”‚       â”œâ”€â”€ If not: broadcast::channel(100)
â”‚       â””â”€â”€ Return sender clone
â”‚
â”œâ”€â”€ StreamingBuffer::new(sender, message_id)
â”‚   â””â”€â”€ Initialize with empty buffer
â”‚
â”œâ”€â”€ Check model availability
â”‚   â””â”€â”€ state.model.read().await
â”‚
â””â”€â”€ If Some(service):
    â”‚
    â””â”€â”€ service.generate_streaming(&message, &mut buffer).await
        â”‚
        â”œâ”€â”€ tokenizer.encode(prompt)?
        â”‚   â”œâ”€â”€ Convert prompt to tokens
        â”‚   â””â”€â”€ Return Vec<u32>
        â”‚
        â”œâ”€â”€ Prepare input tensor
        â”‚   â””â”€â”€ Tensor::new(tokens, &device)?.unsqueeze(0)?
        â”‚
        â””â”€â”€ Generation loop (max_tokens iterations)
            â”‚
            â”œâ”€â”€ model.forward_with_cache(&input_ids)?
            â”‚   â”œâ”€â”€ Embedding lookup
            â”‚   â”œâ”€â”€ Through transformer layers:
            â”‚   â”‚   â”œâ”€â”€ RMSNorm
            â”‚   â”‚   â”œâ”€â”€ Self-Attention (with GQA)
            â”‚   â”‚   â”œâ”€â”€ KV Cache update
            â”‚   â”‚   â”œâ”€â”€ Position encoding (skip on NoPE layers)
            â”‚   â”‚   â”œâ”€â”€ Feed-forward network
            â”‚   â”‚   â””â”€â”€ Residual connections
            â”‚   â””â”€â”€ Output logits
            â”‚
            â”œâ”€â”€ logits_processor.sample(&last_logits)?
            â”‚   â”œâ”€â”€ Apply temperature
            â”‚   â”œâ”€â”€ Apply top_p
            â”‚   â””â”€â”€ Sample token
            â”‚
            â”œâ”€â”€ Check special tokens
            â”‚   â”œâ”€â”€ If think_token_id: enter thinking mode
            â”‚   â”œâ”€â”€ If think_end_token_id: exit thinking mode
            â”‚   â””â”€â”€ If EOS: break loop
            â”‚
            â”œâ”€â”€ tokenizer.decode(&[next_token])?
            â”‚
            â””â”€â”€ buffer.push(token_text).await?
                â”œâ”€â”€ Accumulate in buffer
                â”œâ”€â”€ If 10 tokens OR 500ms elapsed:
                â”‚   â””â”€â”€ flush().await?
                â”‚       â”œâ”€â”€ broadcast::send(StreamEvent::MessageContent)
                â”‚       â””â”€â”€ Reset buffer
                â””â”€â”€ Continue
```

### Fallback Flow (When Model Unavailable)

```
If None (no model):
â”‚
â”œâ”€â”€ Create fallback message
â”‚   â””â”€â”€ "ğŸ”´ **Model not loaded**\n\n..."
â”‚
â”œâ”€â”€ tracing::warn!("No model available for message {}", message_id)
â”‚
â””â”€â”€ Stream fallback word by word
    â”‚
    â””â”€â”€ For each word in fallback.split_whitespace()
        â”œâ”€â”€ buffer.push(&format!("{} ", word)).await?
        â”œâ”€â”€ tokio::time::sleep(30ms).await
        â””â”€â”€ Continue until done
```

---

## UI Input/Output Call Chain

### Complete User Interaction Flow

```
1. USER INPUT PHASE
   Browser
   â”‚
   â”œâ”€â”€ User types in <textarea id="message-input">
   â”œâ”€â”€ User presses Enter (without Shift)
   â””â”€â”€ JavaScript: document.getElementById('chat-form').requestSubmit()

2. HTMX PROCESSING PHASE
   HTMX
   â”‚
   â”œâ”€â”€ Intercept form submission
   â”œâ”€â”€ Prepare POST request
   â”‚   â”œâ”€â”€ URL: /api/chat
   â”‚   â”œâ”€â”€ Data: {session_id, message}
   â”‚   â””â”€â”€ Headers: Content-Type: application/x-www-form-urlencoded
   â””â”€â”€ Send async request

3. SERVER RECEPTION PHASE
   api::send_message()
   â”‚
   â”œâ”€â”€ Extract Form data
   â”œâ”€â”€ Generate message_id (UUID v7)
   â”œâ”€â”€ Log: "Received message: '{}' for session: {}"
   â”‚
   â”œâ”€â”€ Create immediate HTML response
   â”‚   â””â”€â”€ format!(r#"
   â”‚       <div class="message user">
   â”‚           <div class="message-bubble">{escaped_user_message}</div>
   â”‚       </div>
   â”‚       <div class="message assistant" id="msg-{message_id}">
   â”‚           <div class="message-bubble">
   â”‚               <span class="loading">Thinking...</span>
   â”‚           </div>
   â”‚       </div>"#)
   â”‚
   â”œâ”€â”€ Return Html(html) [IMMEDIATE - User sees response]
   â”‚
   â””â”€â”€ tokio::spawn(async move { ... }) [BACKGROUND TASK]

4. BACKGROUND PROCESSING PHASE
   Background Task
   â”‚
   â”œâ”€â”€ Check message type
   â”‚   â”œâ”€â”€ If starts_with("/quote"): stream_quote_buffered()
   â”‚   â””â”€â”€ Else: generate_response_buffered()
   â”‚
   â””â”€â”€ Processing continues...
       [See Inference Engine Call Chain above]

5. EVENT BROADCASTING PHASE
   StreamingBuffer::flush()
   â”‚
   â”œâ”€â”€ Create StreamEvent::MessageContent
   â”œâ”€â”€ broadcast::Sender::send(event)
   â”‚   â””â”€â”€ All subscribers receive copy
   â””â”€â”€ Clear buffer

6. SSE DELIVERY PHASE
   stream_session() handler
   â”‚
   â”œâ”€â”€ BroadcastStream receives event
   â”œâ”€â”€ Map to SSE format
   â”‚   â””â”€â”€ Event::default()
   â”‚       .event("message")
   â”‚       .data(format!("{}|{}", message_id, content))
   â””â”€â”€ Send over SSE connection

7. CLIENT RECEPTION PHASE
   EventSource (Browser)
   â”‚
   â”œâ”€â”€ Receive SSE event
   â”œâ”€â”€ JavaScript: eventSource.addEventListener('message', ...)
   â”œâ”€â”€ Parse: const [messageId, ...contentParts] = e.data.split('|')
   â””â”€â”€ Update DOM
       â”œâ”€â”€ Find: document.querySelector(`#msg-${messageId} .message-bubble`)
       â”œâ”€â”€ Store: messageEl.dataset.rawContent += content
       â””â”€â”€ Display: messageEl.textContent = messageEl.dataset.rawContent

8. COMPLETION & RENDERING PHASE
   On 'complete' event
   â”‚
   â”œâ”€â”€ Get accumulated content: messageEl.dataset.rawContent
   â”œâ”€â”€ Render markdown: marked.parse(rawContent)
   â”œâ”€â”€ Update HTML: messageEl.innerHTML = formattedHtml
   â””â”€â”€ Auto-scroll to bottom
```

---

## Broadcast SSE Architecture

### Why Broadcast Instead of MPSC

```
MPSC Problem:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Producer â”‚â”€â”€â”€â”€â”€>â”‚ Channel  â”‚â”€â”€â”€â”€â”€>â”‚ Consumer â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
                  Once taken, gone!
                  SSE reconnect = lost messages

Broadcast Solution:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Producer â”‚â”€â”€â”€â”€â”€>â”‚ Broadcastâ”‚â”€â”€â”¬â”€â”€>â”‚Consumer 1â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚  Channel â”‚  â”œâ”€â”€>â”‚Consumer 2â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€>â”‚Consumer Nâ”‚
                        â†“
                  Multiple subscribers
                  Late join = get buffered messages
```

### Implementation Details

```rust
// Session creation with broadcast
pub fn create_session(&mut self, session_id: &str) {
    if !self.sessions.contains_key(session_id) {
        let (tx, _rx) = broadcast::channel(100); // 100 message buffer
        
        let session = SessionState {
            id: session_id.to_string(),
            event_sender: tx,
            // ... other fields
        };
        
        self.sessions.insert(session_id.to_string(), session);
    }
}

// SSE subscription
pub fn subscribe(&mut self, session_id: &str) -> Option<broadcast::Receiver<StreamEvent>> {
    self.sessions.get(session_id)
        .map(|s| s.event_sender.subscribe()) // New receiver each time
}

// Broadcasting events
let _ = sender.send(StreamEvent::MessageContent {
    message_id,
    content,
}); // Non-async, returns Result<usize, SendError>
```

### Race Condition Solution

```
Timeline without broadcast (MPSC):
T0: User sends message
T1: Background task starts
T2: Task sends to channel [MESSAGE LOST - no receiver yet]
T3: SSE connects
T4: SSE takes receiver [Too late!]

Timeline with broadcast:
T0: User sends message
T1: Background task starts
T2: Task broadcasts message [BUFFERED in channel]
T3: SSE connects
T4: SSE subscribes [RECEIVES buffered message]
```

---

## Error Handling & Fallback Chains

### Model Loading Failure

```
AppState::new()
â”‚
â””â”€â”€ MLService::new() returns Err
    â”‚
    â”œâ”€â”€ Log: tracing::warn!("âš ï¸ Model not available: {}", e)
    â”œâ”€â”€ Log: tracing::info!("ğŸŒ Server will start without model")
    â”œâ”€â”€ Set: model = None
    â””â”€â”€ Continue startup [SERVER STILL STARTS]
```

### Runtime Model Failure

```
generate_response_buffered()
â”‚
â””â”€â”€ service.generate_streaming() returns Err
    â”‚
    â”œâ”€â”€ Log: tracing::error!("Model generation failed: {}", e)
    â”œâ”€â”€ Create error message with markdown
    â”œâ”€â”€ Stream fallback through buffer
    â””â”€â”€ User sees: "âš ï¸ **Model generation failed**..."
```

### SSE Connection Failure

```
EventSource connection drops
â”‚
â”œâ”€â”€ Browser: Automatic reconnection attempt
â”œâ”€â”€ Server: New subscribe() call
â”œâ”€â”€ Broadcast: New receiver created
â””â”€â”€ Messages: Continue flowing (no loss)
```

### Buffer Overflow Handling

```
broadcast::channel(100) fills up
â”‚
â”œâ”€â”€ Oldest messages dropped (lagged)
â”œâ”€â”€ BroadcastStream receives Err(RecvError::Lagged)
â”œâ”€â”€ Map to StreamEvent::KeepAlive
â””â”€â”€ Continue operation (graceful degradation)
```

---

## Summary

The architecture demonstrates:

1. **Resilient Startup**: Server always starts, model is optional
2. **Event-Driven Flow**: From input to display via broadcast channels
3. **No Message Loss**: Broadcast buffers handle timing issues
4. **Graceful Degradation**: Fallbacks at every level
5. **Clean Separation**: Each layer has clear responsibilities

The system elegantly handles complex async flows while maintaining simplicity and robustness.