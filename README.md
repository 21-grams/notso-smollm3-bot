# SmolLM3 Bot - notso-smollm3-bot

A high-performance Rust chatbot featuring SmolLM3-3B (quantized Q4_K_M) with real-time streaming via HTMX SSE.

## ğŸš€ Features

- **SmolLM3-3B Integration** - Latest Candle.rs ecosystem (>0.9.1)
- **Real-time Streaming** - Character-by-character response streaming
- **HTMX-Powered UI** - Minimal JavaScript, maximum interactivity
- **Markdown Support** - Full markdown rendering for responses
- **Slash Commands** - `/quote`, `/status`, and more
- **Session Management** - Multi-user support with isolated sessions
- **Thinking Mode** - Optional CoT (Chain of Thought) reasoning

## ğŸ“Š Current Status

**Last Major Update**: 2025-01-17
- âœ… Fixed markdown rendering issue with unwanted `<pre><code>` wrapping
- âœ… HTMX SSE streaming with OOB swaps implemented
- âœ… Pure HTMX content routing (no JavaScript accumulation)
- âœ… Proper two-pass markdown rendering (parse then highlight)
- âœ… Clean separation of concerns
- âœ… External JavaScript modules

**Known Issues Fixed**:
- ~~Messages wrapped in code blocks~~ â†’ Fixed by trimming indentation before markdown parsing
- ~~Highlight.js interfering with markdown~~ â†’ Fixed with selective highlighting

## ğŸ—ï¸ Architecture

### Tech Stack
- **Backend**: Rust with Axum web framework
- **ML**: Candle.rs, candle-nn, candle-transformers
- **Frontend**: HTMX 2.0 with SSE extension
- **Streaming**: Server-Sent Events (SSE)
- **Styling**: Minimal CSS with dark theme

### Project Structure
```
notso-smollm3-bot/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.rs              # Application entry point
â”‚   â”œâ”€â”€ state.rs             # Application state management
â”‚   â”œâ”€â”€ config.rs            # Configuration
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ ml/              # ML service with SmolLM3
â”‚   â”‚   â”œâ”€â”€ streaming/       # Streaming buffer implementation
â”‚   â”‚   â””â”€â”€ session/         # Session management
â”‚   â””â”€â”€ web/
â”‚       â”œâ”€â”€ handlers/        # HTTP request handlers
â”‚       â”œâ”€â”€ templates/       # HTML templates
â”‚       â””â”€â”€ static/          # CSS, JavaScript
â”œâ”€â”€ models/                  # GGUF model files
â””â”€â”€ doc/                     # Documentation
```

## ğŸ”§ Setup

### Prerequisites
- Rust 1.75+
- CUDA toolkit (optional, for GPU acceleration)
- 8GB+ RAM for model loading

### Installation

1. Clone the repository:
```bash
git clone https://github.com/21-grams/notso-smollm3-bot
cd notso-smollm3-bot
```

2. Download the model:
```bash
# Place SmolLM3-3B-Q4_K_M.gguf in models/ directory
mkdir models
# Download from HuggingFace or convert your own
```

3. Build and run:
```bash
cargo run --release
```

4. Open browser to `http://localhost:3000`

## ğŸ¯ Development Goals

### Phase 1: Core Functionality âœ…
- [x] Basic chat interface
- [x] SSE streaming
- [x] Markdown rendering
- [x] Session management

### Phase 2: ML Integration (In Progress)
- [ ] GGUF model loading
- [ ] Token streaming
- [ ] Context management
- [ ] Temperature control

### Phase 3: Advanced Features
- [ ] FTS5 search integration
- [ ] Conversation history
- [ ] Model switching
- [ ] Fine-tuning support

## ğŸ“– Documentation

- [HTMX SSE Streaming Solution](doc/HTMX_SSE_Streaming_Solution.md) - Complete streaming architecture
- [SSE Streaming Refactoring Notes](doc/SSE_Streaming_Refactoring_Notes.md) - Future improvements
- [Pure HTMX SSE Implementation](doc/Pure_HTMX_SSE_Implementation.md) - Implementation details

## ğŸ¤ Collaboration Guidelines

### Development Rules
- **Build**: `cargo run` - Create environment setup scripts for dependencies
- **Testing**: Unit tests for core features only
- **Documentation**: Use `///` comments, maintain /doc folder
- **Safety**: Pure safe Rust preferred, justify any `unsafe` blocks
- **Clean Code**: No test scripts without permission, ask clear questions

### Contribution Process
1. Check existing issues/discussions
2. Create feature branch
3. Follow Rust best practices
4. Update documentation
5. Submit PR with clear description

## ğŸ” Technical Highlights

### HTMX SSE with OOB Swaps
The streaming solution uses HTMX's out-of-band swaps to route content to specific message bubbles:

```rust
// Backend sends targeted content
Event::default()
    .event("message")
    .data(format!(
        r#"<span hx-swap-oob="beforeend:#msg-{}-content">{}</span>"#,
        message_id, content
    ))
```

```html
<!-- Frontend receives and auto-routes content -->
<div id="msg-{id}-content"></div>
```

### Minimal JavaScript
JavaScript is only used for:
- Markdown rendering (marked.js)
- UI helpers (auto-scroll, textarea resize)
- No message accumulation or routing needed!

## ğŸ“ˆ Performance

- **Streaming Latency**: <50ms per chunk
- **Memory Usage**: ~500MB (without model)
- **Concurrent Sessions**: 100+ supported
- **Model Inference**: TBD (pending GGUF integration)

## ğŸ› Known Issues

- Model loading not yet implemented (fallback messages active)
- FTS5 search integration pending
- Token counting not active

## ğŸ“ License

MIT License - See LICENSE file for details

## ğŸ™ Acknowledgments

- HTMX team for the excellent framework
- Candle.rs team for the ML ecosystem
- SmolLM team for the model

---

**Project Status**: ğŸŸ¡ Active Development

For questions or contributions, please open an issue on [GitHub](https://github.com/21-grams/notso-smollm3-bot).