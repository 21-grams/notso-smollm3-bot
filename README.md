# NotSo-SmolLM3 Bot

A high-performance, production-ready chatbot implementation of SmolLM3-3B using pure Rust and the Candle machine learning framework. This project demonstrates efficient quantized inference with streaming responses, all running natively without Python dependencies.

## ğŸ¯ Project Goal

To create a fully-featured SmolLM3 inference system in Rust that:
- Runs Q4_K_M quantized models efficiently on consumer hardware
- Provides real-time token streaming with intelligent buffering
- Supports SmolLM3's unique features (thinking mode, GQA, NoPE layers)
- Delivers a smooth chat experience through a clean web interface
- Serves as a reference implementation for Rust-based LLM inference

## ğŸ¯ Target Model

**SmolLM3-3B Q4_K_M GGUF** - Specifically optimized for:
- **4-bit quantization** with K-means clustering for quality preservation
- **~1.5GB model size** - fits in consumer GPU memory
- **Direct quantized operations** - no dequantization overhead
- **Efficient inference** - maintains original model's optimization intent

The Q4_K_M format provides the best balance of model quality, inference speed, and memory efficiency for the 3B parameter SmolLM3 model.

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Web Layer                              â”‚
â”‚                    (Axum 0.8 + HTMX)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  â€¢ HTTP Routes      â€¢ SSE Streaming                 â”‚   â”‚
â”‚  â”‚  â€¢ Chat UI          â€¢ Slash Commands                â”‚   â”‚
â”‚  â”‚  â€¢ Session Mgmt     â€¢ Markdown Rendering (Client)   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Service Layer                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  â€¢ ML Service       â€¢ Streaming Buffer              â”‚   â”‚
â”‚  â”‚  â€¢ Session Manager  â€¢ Template Engine               â”‚   â”‚
â”‚  â”‚  â€¢ Event Pipeline   â€¢ Command Handlers              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ML Foundation Layer                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Official Candle â”‚  SmolLM3 Adapter â”‚  Quantized   â”‚   â”‚
â”‚  â”‚  Integration     â”‚  & Extensions    â”‚  Operations  â”‚   â”‚
â”‚  â”‚                  â”‚                  â”‚              â”‚   â”‚
â”‚  â”‚  â€¢ GGUF Loader   â”‚  â€¢ Thinking Mode â”‚  â€¢ Q4_K_M    â”‚   â”‚
â”‚  â”‚  â€¢ Model Weights â”‚  â€¢ KV Cache (GQA)â”‚  â€¢ Direct    â”‚   â”‚
â”‚  â”‚  â€¢ Forward Pass  â”‚  â€¢ NoPE Layers   â”‚    MatMul    â”‚   â”‚
â”‚  â”‚  â€¢ Device Mgmt   â”‚  â€¢ Chat Template â”‚  â€¢ No Deq.   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
src/
â”œâ”€â”€ main.rs                 # Application entry point
â”œâ”€â”€ config.rs              # Global configuration
â”œâ”€â”€ state.rs               # Application state management
â”œâ”€â”€ lib.rs                 # Library exports
â”‚
â”œâ”€â”€ web/                   # Web Layer
â”‚   â”œâ”€â”€ handlers/
â”‚   â”‚   â”œâ”€â”€ api.rs        # Main API endpoints & command routing
â”‚   â”‚   â”œâ”€â”€ chat.rs       # Chat page rendering
â”‚   â”‚   â”œâ”€â”€ commands.rs   # Slash command handlers
â”‚   â”‚   â””â”€â”€ health.rs     # Health check endpoint
â”‚   â”œâ”€â”€ routes.rs         # Route definitions
â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â””â”€â”€ chat.html     # Single unified chat template
â”‚   â””â”€â”€ static/
â”‚       â”œâ”€â”€ css/          # Neumorphic UI styles
â”‚       â””â”€â”€ js/           # Slash commands & chat logic
â”‚
â”œâ”€â”€ services/              # Service Layer
â”‚   â”œâ”€â”€ ml/
â”‚   â”‚   â”œâ”€â”€ service.rs    # ML service orchestration
â”‚   â”‚   â”œâ”€â”€ official/     # Candle integration
â”‚   â”‚   â”‚   â”œâ”€â”€ model.rs  # SmolLM3 model wrapper
â”‚   â”‚   â”‚   â”œâ”€â”€ gguf_loader.rs  # GGUF metadata mapping
â”‚   â”‚   â”‚   â””â”€â”€ config.rs # Model configuration
â”‚   â”‚   â””â”€â”€ smollm3/      # SmolLM3-specific features
â”‚   â”‚       â”œâ”€â”€ adapter.rs       # Model adapter
â”‚   â”‚       â”œâ”€â”€ chat_template.rs # Chat formatting
â”‚   â”‚       â”œâ”€â”€ thinking.rs      # Thinking mode
â”‚   â”‚       â”œâ”€â”€ kv_cache.rs      # GQA-optimized cache
â”‚   â”‚       â”œâ”€â”€ nope_layers.rs   # Position encoding
â”‚   â”‚       â””â”€â”€ generation.rs    # Token generation
â”‚   â”œâ”€â”€ streaming/
â”‚   â”‚   â”œâ”€â”€ buffer.rs     # Unified streaming buffer
â”‚   â”‚   â””â”€â”€ sse_handler.rs # SSE event handling
â”‚   â”œâ”€â”€ session.rs        # Session management
â”‚   â””â”€â”€ template/         # Template rendering
â”‚
â””â”€â”€ types/                 # Shared types
    â”œâ”€â”€ events.rs         # Stream event types
    â”œâ”€â”€ message.rs        # Message structures
    â””â”€â”€ session.rs        # Session types
```

## ğŸ”„ Data Flow

### 1. **User Input Flow**
```
User Input â†’ Web Handler â†’ Command Detection â†’ Service Layer
                                â†“
                        Regular Chat / Slash Command
                                â†“
                         Unified Processing
```

### 2. **Streaming Response Flow**
```
Model/Command Output â†’ Streaming Buffer â†’ SSE Events â†’ Client
         â†“                    â†“              â†“           â†“
   Token Generation    Accumulate (10)   "message"   Markdown
                          or 100ms        event      Rendering
```

### 3. **Buffer Strategy**
- Accumulates tokens until threshold (10 tokens or 100ms)
- Reduces DOM updates for smooth user experience
- Single buffer implementation for all output types
- Automatic completion signaling

## ğŸš€ Key Features

### SmolLM3-Specific Optimizations
- **Grouped Query Attention (GQA)**: 4:1 KV head ratio for 75% memory savings
- **NoPE Layers**: Layers [3,7,11,15,19,23,27,31,35] skip position encoding
- **Thinking Mode**: Native `<think>` token support for reasoning
- **128k Vocabulary**: Extended tokenizer for better coverage

### Technical Highlights
- **Pure Rust**: No Python dependencies, fully native
- **Q4_K_M Quantization**: 4-bit weights with minimal quality loss
- **Direct Quantized Ops**: No dequantization overhead
- **Unified Streaming**: Single pipeline for all content types
- **HTMX + SSE**: Minimal JavaScript, server-driven UI

## ğŸ­ Modular Design Philosophy

### Leveraging Official Candle Ecosystem
The project maximizes use of official Candle (v0.9.1+) and Tokenizers (v0.21+) crates:

```rust
// Official Candle components we build upon:
candle_core::quantized::gguf_file     // GGUF file handling
candle_core::quantized::QMatMul       // Quantized matrix operations
candle_transformers::models::quantized_llama  // Base model architecture
tokenizers::Tokenizer                 // HuggingFace tokenizers
```

### SmolLM3 Adaptation Layer
Model-specific features are cleanly isolated in `src/services/ml/smollm3/`:
- **Extends** official Candle without modifying it
- **Adapts** generic Llama to SmolLM3's architecture
- **Preserves** quantization throughout inference
- **Contained** in a single directory for maintainability

### Quantization Preservation
```rust
// We maintain quantized weights throughout:
QTensor â†’ QMatMul â†’ Quantized Output
        â†‘
   No dequantization!
```

This design ensures:
1. **Maximum compatibility** with Candle updates
2. **Optimal performance** from quantized operations
3. **Clean separation** of concerns
4. **Easy maintenance** as Candle evolves

## ğŸ› ï¸ Setup & Installation

### Prerequisites
- Rust 1.75+ (for async trait support)
- 4GB+ RAM
- (Optional) CUDA 12+ or Metal for GPU acceleration

### Required Model Files
The system specifically requires:
- **Model**: `SmolLM3-3B-Q4_K_M.gguf` (~1.5GB)
- **Tokenizer**: `tokenizer.json` from SmolLM3-3B
- **Format**: GGUF with Q4_K_M quantization (4-bit)

### Quick Start
```bash
# Clone the repository
git clone https://github.com/21-grams/notso-smollm3-bot.git
cd notso-smollm3-bot

# Download model files
cd models
wget https://huggingface.co/HuggingFaceTB/SmolLM3-3B-Q4_K_M.gguf
wget https://huggingface.co/HuggingFaceTB/SmolLM3-3B/tokenizer.json
cd ..

# Build and run
cargo run --release

# For GPU support
cargo run --release --features cuda  # NVIDIA
cargo run --release --features metal # Apple Silicon
```

Access the chat interface at `http://localhost:3000`

## ğŸ“Š Performance Targets

- **First Token Latency**: < 500ms
- **Generation Speed**: 1-2 tokens/second
- **Memory Usage**: ~2GB with Q4_K_M (1.5GB model + overhead)
- **Context Length**: 2048 tokens (expandable)
- **Streaming Buffer**: 10 tokens or 100ms flush
- **Quantization**: Maintained throughout inference (no dequantization)

## ğŸ”§ Configuration

Key settings in `config.rs`:
- Model paths and device selection
- Temperature and sampling parameters
- Buffer thresholds for streaming
- Thinking mode defaults

## ğŸ§ª Testing

The `/quote` command serves as an integration test for the streaming pipeline:
- Tests buffer accumulation and flushing
- Validates SSE event flow
- Demonstrates markdown rendering
- Confirms proper stream completion

## ğŸ“ License

MIT

## ğŸ™ Acknowledgments

- [Candle](https://github.com/huggingface/candle) - Rust ML framework
- [HuggingFace](https://huggingface.co/HuggingFaceTB/SmolLM3-3B) - SmolLM3 models
- [Axum](https://github.com/tokio-rs/axum) - Web framework
- [HTMX](https://htmx.org) - Hypermedia-driven UI
