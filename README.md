# NotSo-SmolLM3 Bot

Production-ready SmolLM3 chatbot using Candle.rs with a clean 3-tier architecture and neumorphic UI.

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Web Layer (HTMX)                      â”‚
â”‚                 Neumorphic UI, SSE Streaming             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Service Layer                           â”‚
â”‚         ML Orchestration, Session Management             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             Inference Foundation Layer                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Low-level   â”‚  â”‚  Generation  â”‚  â”‚   Services   â”‚  â”‚
â”‚  â”‚   Candle     â”‚  â”‚     Loop     â”‚  â”‚  ML/SmolLM3  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Latest Update (Phase: Web Server Route Fix)

### **Fixed Route Conflict Issue** âœ…
Resolved the Axum router conflict where `/static` route was being registered twice:

**Problem**: 
```
thread 'main' panicked at src/web/server.rs:22:10:
Invalid route "/static/{*__private__axum_nest_tail_param}":
Insertion failed due to conflict with previously registered route
```

**Root Cause**:
- Static file route was defined in both `routes.rs` and `server.rs`
- Axum doesn't allow duplicate route patterns

**Solution**:
1. Removed duplicate `/static` registration from `src/web/routes.rs`
2. Kept single registration in `src/web/server.rs` for clarity
3. Added documentation comments for middleware stack order
4. Created `/doc/routing-architecture.md` for routing best practices

### **Architecture Improvements**
- **Clear Separation**: Static files served at app level, application routes in dedicated module
- **Single Responsibility**: Each module handles specific route types
- **Documentation**: Added routing architecture guide for future reference

## ğŸ“ Project Structure & File Descriptions

```
notso-smollm3-bot/
â”œâ”€â”€ doc/                           # Documentation
â”‚   â”œâ”€â”€ architecture.md           # System design and architecture decisions
â”‚   â”œâ”€â”€ implementation_status.md  # Current progress and roadmap
â”‚   â”œâ”€â”€ candle_reference.md       # Candle.rs patterns and examples
â”‚   â””â”€â”€ ui_ux_interaction.md      # UI/UX flow documentation
â”‚
â”œâ”€â”€ models/                        # Model storage (gitignored)
â”‚   â”œâ”€â”€ download.sh               # Script to download GGUF models
â”‚   â””â”€â”€ [*.gguf, *.json]         # Model and tokenizer files
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ inference/                # Inference Foundation Layer [NEW]
â”‚   â”‚   â”œâ”€â”€ candle/              # Low-level Candle operations
â”‚   â”‚   â”‚   â”œâ”€â”€ device.rs        # Device management & memory tracking
â”‚   â”‚   â”‚   â”œâ”€â”€ kv_cache.rs      # KV cache tensor operations
â”‚   â”‚   â”‚   â”œâ”€â”€ tensor_ops.rs    # Common tensor utilities
â”‚   â”‚   â”‚   â”œâ”€â”€ model_loader.rs  # GGUF model loading
â”‚   â”‚   â”‚   â”œâ”€â”€ quantized_ops.rs # Quantized operations
â”‚   â”‚   â”‚   â””â”€â”€ mod.rs           # Module exports
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ generation.rs        # Core generation loop
â”‚   â”‚   â”œâ”€â”€ engine.rs            # Inference engine orchestration
â”‚   â”‚   â””â”€â”€ mod.rs               # Inference exports
â”‚   â”‚
â”‚   â”œâ”€â”€ web/                      # Web Layer - Self-contained UI
â”‚   â”‚   â”œâ”€â”€ static/
â”‚   â”‚   â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ main.css    # Global neumorphic design system
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ chat.css    # Chat-specific neumorphic styles
â”‚   â”‚   â”‚   â””â”€â”€ js/
â”‚   â”‚   â”‚       â””â”€â”€ chat.js     # HTMX SSE handlers, markdown rendering
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â”‚   â”œâ”€â”€ base.html       # Base template with layout
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.html       # Main chat interface
â”‚   â”‚   â”‚   â””â”€â”€ components/     # Reusable components
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ handlers/            # HTTP Request Handlers
â”‚   â”‚   â”‚   â”œâ”€â”€ mod.rs          # Module exports
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.rs         # Chat endpoints (POST /api/chat)
â”‚   â”‚   â”‚   â”œâ”€â”€ api.rs          # API endpoints (thinking toggle, context)
â”‚   â”‚   â”‚   â”œâ”€â”€ sse.rs          # SSE streaming endpoint
â”‚   â”‚   â”‚   â””â”€â”€ health.rs       # Health check endpoint
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ server.rs            # Axum server configuration
â”‚   â”‚   â”œâ”€â”€ routes.rs            # Route definitions and middleware
â”‚   â”‚   â””â”€â”€ mod.rs               # Web module exports
â”‚   â”‚
â”‚   â”œâ”€â”€ services/                 # Service Layer - Business Logic
â”‚   â”‚   â”œâ”€â”€ ml/                  # Machine Learning Services
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”œâ”€â”€ official/        # Official Candle Foundation
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ model.rs    # OfficialSmolLM3Model - quantized_llama wrapper
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ config.rs   # SmolLM3Config - model parameters
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ loader.rs   # OfficialLoader - GGUF file loading
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ device.rs   # DeviceManager - high-level device selection
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ mod.rs      # Module exports
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”œâ”€â”€ smollm3/         # SmolLM3-Specific Extensions
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ adapter.rs      # SmolLM3Adapter - bridges to official
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ generation.rs   # SmolLM3Generator - uses inference layer
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ thinking.rs     # ThinkingDetector - <think> token handling
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ kv_cache.rs     # KVCache - SmolLM3-specific cache logic
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ nope_layers.rs  # NopeHandler - NoPE layer management
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ tokenizer_ext.rs # SmolLM3TokenizerExt - chat templates
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ stub_mode.rs    # StubModeService - testing without models
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ mod.rs          # Module exports
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”œâ”€â”€ streaming/       # Real-time Streaming
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ buffer.rs   # ResponseBuffer - token batching
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ events.rs   # StreamEvent - SSE event types
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ pipeline.rs # StreamingPipeline - orchestration
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ mod.rs      # Module exports
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”œâ”€â”€ service.rs      # MLService - high-level orchestration
â”‚   â”‚   â”‚   â””â”€â”€ mod.rs          # ML module exports
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ template/            # Template Rendering
â”‚   â”‚   â”‚   â”œâ”€â”€ engine.rs       # TemplateEngine - MiniJinja setup
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.rs         # Chat-specific templates
â”‚   â”‚   â”‚   â””â”€â”€ mod.rs          # Module exports
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ session.rs          # Session management
â”‚   â”‚   â”œâ”€â”€ streaming.rs        # Streaming service
â”‚   â”‚   â”œâ”€â”€ metrics.rs          # Performance metrics
â”‚   â”‚   â””â”€â”€ mod.rs              # Services module exports
â”‚   â”‚
â”‚   â”œâ”€â”€ types/                   # Shared Type Definitions
â”‚   â”‚   â”œâ”€â”€ events.rs           # StreamEvent, GenerationEvent types
â”‚   â”‚   â”œâ”€â”€ message.rs          # Message, ChatMessage structures
â”‚   â”‚   â”œâ”€â”€ session.rs          # Session, ChatSession types
â”‚   â”‚   â”œâ”€â”€ errors.rs           # Error types
â”‚   â”‚   â””â”€â”€ mod.rs              # Type exports
â”‚   â”‚
â”‚   â”œâ”€â”€ smollm3/                # SmolLM3 model core
â”‚   â”‚   â”œâ”€â”€ model.rs            # Model implementation
â”‚   â”‚   â”œâ”€â”€ config.rs           # Model configuration
â”‚   â”‚   â”œâ”€â”€ tokenizer.rs        # Tokenizer handling
â”‚   â”‚   â”œâ”€â”€ chat_template.rs    # Chat formatting
â”‚   â”‚   â””â”€â”€ mod.rs              # Module exports
â”‚   â”‚
â”‚   â”œâ”€â”€ config.rs                # Application configuration
â”‚   â”œâ”€â”€ state.rs                 # AppState - shared application state
â”‚   â”œâ”€â”€ lib.rs                   # Library exports
â”‚   â””â”€â”€ main.rs                  # Entry point - server initialization
â”‚
â”œâ”€â”€ tests/                        # Test Suite
â”‚   â”œâ”€â”€ integration/             # Integration tests
â”‚   â””â”€â”€ unit/                    # Unit tests
â”‚
â”œâ”€â”€ Cargo.toml                   # Project dependencies and metadata
â”œâ”€â”€ Cargo.lock                   # Locked dependency versions
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ build.sh                     # Production build script
â”œâ”€â”€ run.sh                       # Run script with environment setup
â””â”€â”€ test_build.sh                # Quick test build script
```

## ğŸ¯ Logical Organization

### **Three-Tier Architecture**

1. **Web Layer** (`/src/web`)
   - Self-contained UI with all assets
   - Handles HTTP requests and responses
   - Manages SSE streaming connections
   - No business logic, only presentation

2. **Service Layer** (`/src/services`)
   - Business logic and orchestration
   - ML model management
   - Session and state management
   - Template rendering

3. **Inference Foundation** (`/src/inference`)
   - **Candle**: Low-level tensor operations
   - **Generation**: Core generation loop
   - **Engine**: Inference orchestration

### **Key Design Principles**

- **Modularity**: Each component has a single responsibility
- **Layered Architecture**: Clear boundaries between layers
- **Dependency Injection**: Services receive dependencies via constructors
- **Type Safety**: Strong typing throughout with shared type definitions
- **Progressive Enhancement**: HTMX for interactivity without heavy JS

## ğŸš€ Quick Start

```bash
# Setup
chmod +x *.sh
./test_build.sh  # Verify structure

# Download models (optional)
cd models && ./download.sh && cd ..

# Build & Run
cargo build --release
./run.sh

# Visit http://localhost:3000
```

## ğŸ”§ Configuration

### Model Specifications
- **Architecture**: SmolLM3-3B with 4-group GQA
- **Quantization**: Q4_K_M (4-bit)
- **Context**: 2048 tokens (expandable to 32K)
- **Layers**: 36 with NoPE on layers [3,7,11,15,19,23,27,31,35]
- **Performance**: Target 1-2 tok/s
- **Memory**: ~2GB GPU RAM with KV cache

### Environment Variables
```bash
RUST_LOG=info                  # Logging level
MODEL_PATH=models/model.gguf   # Path to GGUF model
TOKENIZER_PATH=models/tokenizer.json  # Path to tokenizer
PORT=3000                       # Server port
```

## ğŸ“Š Implementation Status

### âœ… Complete
- Web UI with neumorphic design
- HTMX + SSE streaming
- Service layer architecture
- ML foundation structure
- Session management
- Template engine
- **Inference layer (NEW)**
  - Device management
  - KV cache operations
  - Tensor utilities
  - Generation loop

### ğŸš§ In Progress
- Model integration with inference layer
- Generation pipeline optimization
- Tool use system
- Follow-up suggestions

### ğŸ“‹ Planned
- Context management improvements
- Performance optimization
- Persistent sessions
- Multi-model support
- Distributed inference

## ğŸ“š Documentation

- `/doc/architecture.md` - System design decisions
- `/doc/implementation_status.md` - Detailed progress tracking
- `/doc/candle_reference.md` - Candle.rs usage patterns
- `/doc/ui_ux_interaction.md` - User interaction flow

## ğŸ› ï¸ Development Commands

```bash
# Build with all features
cargo build --release

# Run with environment setup
./run.sh

# Test compilation only
./test_compilation.sh

# Fix common issues
./fix_compilation.sh

# Make all scripts executable
./make_all_executable.sh
```

## ğŸ“„ License

MIT
